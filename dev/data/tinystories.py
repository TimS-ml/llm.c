"""
TinyStories Dataset Preparation for Language Model Training

This script downloads and tokenizes the TinyStories dataset, a collection of
synthetic short stories generated by GPT-3.5/GPT-4, designed for training
small language models.

Dataset Information:
    - Source: HuggingFace (roneneldan/TinyStories)
    - Size: ~2GB compressed, ~50 JSON shards
    - Total Stories: ~2.1 million synthetic stories
    - Content: Simple stories with vocabulary suitable for 3-4 year olds
    - Purpose: Training small LMs with limited compute resources

Processing Steps:
    1. Downloads compressed dataset from HuggingFace (~2GB tar.gz)
    2. Extracts 50 JSON shards containing stories
    3. Processes shards in parallel using multiprocessing
    4. Shuffles stories within each shard (deterministic seed)
    5. Tokenizes using GPT-2 or LLaMA-3 tokenizer
    6. Splits: Shard 0 = validation, shards 1-49 = training
    7. Saves as binary files for C/CUDA loading

Output Files:
    - tinystories/TinyStories_all_data.tar.gz: Compressed dataset
    - tinystories/TinyStories_all_data/*.json: 50 extracted shards
    - tinystories/TinyStories_val.bin: Validation tokens
    - tinystories/TinyStories_train.bin: Training tokens

Usage Examples:
    # Process with GPT-2 tokenizer (default)
    $ python dev/data/tinystories.py --model=gpt-2
    Number of shards: 50
    Tokenizing val split...
    writing 19,043,638 tokens to tinystories/TinyStories_val.bin
    Tokenizing train split...
    writing 925,653,391 tokens to tinystories/TinyStories_train.bin

    # Process with LLaMA-3 tokenizer
    $ python dev/data/tinystories.py --model=llama-3
    Number of shards: 50
    Tokenizing val split...
    writing 18,660,516 tokens to tinystories/TinyStories_val.bin
    Tokenizing train split...
    writing 907,021,844 tokens to tinystories/TinyStories_train.bin

Performance:
    Runs in a few minutes depending on internet connection and CPU cores.
    Uses multiprocessing for parallel shard processing.

Data Format:
    - Input: JSON files with {"story": "text..."} format
    - Output: Binary files with uint16 (GPT-2) or uint32 (LLaMA-3) tokens
    - Each story is preceded by an end-of-text token delimiter

Dataset Statistics (GPT-2):
    - Validation: ~19M tokens (1 shard)
    - Training: ~926M tokens (49 shards)
    - Total: ~945M tokens
"""

import argparse
import os
import glob
import json
import random
from concurrent.futures import ProcessPoolExecutor, as_completed

import tiktoken
from transformers import AutoTokenizer

from data_common import download_file, write_datafile

# -----------------------------------------------------------------------------
# Directory where dataset will be cached (sibling to this script)
DATA_CACHE_DIR = os.path.join(os.path.dirname(__file__), "tinystories")

def download():
    """
    Download and extract the TinyStories dataset from HuggingFace.

    Downloads a ~2GB compressed tar.gz file containing 50 JSON shards of stories.
    If files already exist, skips the download/extraction to save time.

    Returns:
        None

    Side Effects:
        - Creates DATA_CACHE_DIR if it doesn't exist
        - Downloads TinyStories_all_data.tar.gz (~2GB)
        - Extracts to TinyStories_all_data/ directory (50 JSON files)
        - Prints number of shards found

    Performance:
        Download takes a few minutes depending on internet speed.
        Extraction takes 30-60 seconds.
    """
    # Create cache directory if it doesn't exist
    os.makedirs(DATA_CACHE_DIR, exist_ok=True)

    # Download the compressed dataset if not already present
    data_url = "https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz"
    data_filename = os.path.join(DATA_CACHE_DIR, "TinyStories_all_data.tar.gz")
    if not os.path.exists(data_filename):
        print(f"Downloading {data_url} to {data_filename}...")
        download_file(data_url, data_filename)
    else:
        print(f"{data_filename} already exists, skipping download...")

    # Extract the tar.gz file into JSON shards
    data_dir = os.path.join(DATA_CACHE_DIR, "TinyStories_all_data")
    if not os.path.exists(data_dir):
        os.makedirs(data_dir, exist_ok=True)
        print(f"Unpacking {data_filename}...")
        # Extract all JSON files from the archive
        os.system(f"tar -xzf {data_filename} -C {data_dir}")
    else:
        print(f"{data_dir} already exists, skipping unpacking...")

    # Verify extraction and report dataset statistics
    shard_filenames = sorted(glob.glob(os.path.join(data_dir, "*.json")))
    print("Download done.")
    print(f"Number of shards: {len(shard_filenames)}")

    # Optional: Print example story for verification (commented out for cleaner output)
    # with open(shard_filenames[0], "r") as f:
    #     data = json.load(f)
    # print(f"Example story:\n{data[0]}")

def process_shard(shard_index, shard_filename, model_desc):
    """
    Process a single JSON shard: load, shuffle, and tokenize all stories.

    This function is designed to be called in parallel by multiple worker processes.
    It loads stories from a JSON file, shuffles them deterministically, and tokenizes
    each story with an end-of-text delimiter.

    Args:
        shard_index: Index of this shard (used for deterministic shuffling)
        shard_filename: Path to the JSON file containing stories
        model_desc: Model type, either "gpt-2" or "llama-3"

    Returns:
        List of token IDs from all stories in this shard

    Raises:
        ValueError: If model_desc is not "gpt-2" or "llama-3"

    Note:
        - Shuffling uses deterministic seed (1337 + shard_index) for reproducibility
        - Each story is preceded by an EOT token to mark document boundaries
        - Leading/trailing whitespace is stripped from stories before tokenization
    """
    # Initialize the appropriate tokenizer based on model type
    if model_desc == "gpt-2":
        # Use tiktoken for fast GPT-2 tokenization
        enc = tiktoken.get_encoding("gpt2")
        encode = lambda s: enc.encode_ordinary(s)
        eot = enc._special_tokens['<|endoftext|>']  # End-of-text token (50256)
    elif model_desc == "llama-3":
        # Use HuggingFace transformers for LLaMA-3 tokenization
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3.1-8B")
        encode = lambda s: tokenizer.encode(s, add_special_tokens=False, verbose=False, split_special_tokens=True)
        eot = tokenizer.encode('')[0]  # EOT token for LLaMA-3 (128000)
    else:
        raise ValueError(f"unknown model descriptor {model_desc}")

    # Load all stories from this shard
    with open(shard_filename, "r") as f:
        data = json.load(f)  # List of dicts with "story" key

    # Shuffle stories deterministically (seed = 1337 + shard_index)
    # This ensures reproducible shuffling across runs while varying between shards
    rng = random.Random(1337 + shard_index)
    rng.shuffle(data)

    # Tokenize all stories in this shard
    all_tokens = []
    for example in data:
        # Extract story text and clean whitespace
        text = example["story"]
        text = text.strip()  # Remove leading/trailing whitespace

        # Tokenize the story
        tokens = encode(text)

        # Add end-of-text token before the story to mark document boundary
        all_tokens.append(eot)
        all_tokens.extend(tokens)

    return all_tokens

def tokenize(model_desc):
    """
    Tokenize all TinyStories shards and create train/validation splits.

    This function processes all 50 JSON shards using parallel processing.
    Shard 0 becomes the validation set, and shards 1-49 become the training set.
    Each shard is processed independently in parallel for efficiency.

    Args:
        model_desc: Model type, either "gpt-2" or "llama-3"

    Returns:
        None

    Side Effects:
        - Creates TinyStories_val.bin (~19M tokens for GPT-2)
        - Creates TinyStories_train.bin (~926M tokens for GPT-2)

    Performance:
        Uses ProcessPoolExecutor for parallel shard processing.
        Runtime: 2-5 minutes depending on CPU cores and model type.

    Data Split:
        - Validation: Shard 0 only (~1/50 = 2% of data)
        - Training: Shards 1-49 (~49/50 = 98% of data)

    Note:
        The validation split is smaller than typical (2% vs 10%) because
        the dataset is large enough that 2% provides sufficient validation data.
    """
    # Get all JSON shard files
    data_dir = os.path.join(DATA_CACHE_DIR, "TinyStories_all_data")
    shard_filenames = sorted(glob.glob(os.path.join(data_dir, "*.json")))

    # Split shards: first shard for validation, rest for training
    val_shards = [shard_filenames[0]]
    train_shards = shard_filenames[1:]

    # Process each split (validation and training)
    for split_name, split_shards in [("val", val_shards), ("train", train_shards)]:
        print(f"Tokenizing {split_name} split...")

        # Collect tokens from all shards in this split
        all_tokens = []

        # Use multiprocessing to process shards in parallel
        # Each worker process handles one shard independently
        with ProcessPoolExecutor() as executor:
            # Submit all shard processing jobs
            futures = [executor.submit(process_shard, shard_index, shard_filename, model_desc)
                       for shard_index, shard_filename in enumerate(split_shards)]

            # Collect results as they complete (order doesn't matter for training data)
            for future in as_completed(futures):
                all_tokens.extend(future.result())

        # Write all tokens to a single binary file
        split_filename = os.path.join(DATA_CACHE_DIR, f"TinyStories_{split_name}.bin")
        write_datafile(split_filename, all_tokens, model_desc)

if __name__ == "__main__":
    # Command-line interface for dataset preparation
    parser = argparse.ArgumentParser(description="Tiny Stories dataset preprocessing")
    parser.add_argument("-m", "--model_desc", type=str, default="gpt-2",
                        choices=["gpt-2", "llama-3"],
                        help="Model type, gpt-2|llama-3")
    args = parser.parse_args()

    # Execute the pipeline: download, extract, then tokenize
    download()
    tokenize(args.model_desc)